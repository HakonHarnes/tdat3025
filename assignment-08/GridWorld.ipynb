{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2A - GridWorld\n",
    "Lag et enkelt gridworld-environment. Dette innebærer at environmentet har et\n",
    "diskret rutenett, og at en agent kan bevege seg rundt med fire handlinger (opp,\n",
    "ned, høyre, venstre). Simuleringen terminerer når agenten har nådd et plassert\n",
    "mål-posisjon som gir reward 1. Om man ønsker, kan det legges inn f.eks. solide\n",
    "vegger eller farlige områder som gir straff rundt omkring. Environmentet skal\n",
    "ha samme interface som cartpole (.step(a)-funksjon, og .reset())\n",
    "\n",
    "Deretter skal implementasjonen av Q-læring fra forrige oppgave brukes for å\n",
    "trene en agent i environmentet. Til slutt skal Q-verdiene visualiserer inne i selve\n",
    "environmentet, og dette kan gjøres på flere måter. En måte erå fargelegge rutene\n",
    "basert på den høyeste Q-verdien fra tilsvarende rad i Q-tabellen. Alternativt så\n",
    "kan man tegne inn piler som peker i samme retning som handlingen med høyest\n",
    "Q-verdi.\n",
    "\n",
    "Tips: Biblioteket pygame er veldig greit for å lage visualisering av environmentet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym \n",
    "import math \n",
    "import numpy as np \n",
    "from gridworld import GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = GridWorld(800, 64, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters \n",
    "BUCKETS = (8, 8) \n",
    "EPISODES = 3000\n",
    "MIN_LEARNING_RATE = 0.1\n",
    "MIN_EPSILON = 0.1\n",
    "DISCOUNT = 1.0\n",
    "DECAY = 500\n",
    "\n",
    "# Visualization variables \n",
    "SHOW_STATS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q_table = np.zeros(BUCKETS + (env.action_space.n, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "upper_bounds = [env.observation_space.high[0], 0.5, env.observation_space.high[1], math.radians(50) / 1.]\n",
    "lower_bounds = [env.observation_space.low[0], -0.5, env.observation_space.low[1], -math.radians(50) / 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Discretizes the state \n",
    "def discretize_state(obs):\n",
    "    discretized = list()\n",
    "    \n",
    "    for i in range(len(obs)):\n",
    "        scaling = (obs[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i])\n",
    "        new_obs = int(round((BUCKETS[i] - 1) * scaling))\n",
    "        new_obs = min(BUCKETS[i] - 1, max(0, new_obs))\n",
    "        discretized.append(new_obs)\n",
    "        \n",
    "    return tuple(discretized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Chooses what action to take (random or look in Q-Table)\n",
    "def choose_action(state):\n",
    "    if (np.random.random() < epsilon):\n",
    "        return env.action_space.sample() # Random action\n",
    "    else:\n",
    "        return np.argmax(q_table[state]) # Looks up in the Q-Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Updates the Q-Table \n",
    "def update_q(state, action, reward, new_state):\n",
    "    q_table[state][action] += learning_rate * (reward + DISCOUNT * np.max(q_table[new_state]) - q_table[state][action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Updates epsilon value (logarithmically decreasing)\n",
    "def get_epsilon(episode):\n",
    "    return max(MIN_EPSILON, min(1., 1. - math.log10((episode + 1) / DECAY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates the learning rate (logarithmically decreasing)\n",
    "def get_learning_rate(episode):\n",
    "    return max(MIN_LEARNING_RATE, min(1., 1. - math.log10((episode + 1) / DECAY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  Score\n",
      "500\t 8.2%\n",
      "1000\t 44.4%\n",
      "1500\t 92.6%\n",
      "2000\t 98.2%\n",
      "2500\t 99.8%\n",
      "3000\t 100.0%\n"
     ]
    }
   ],
   "source": [
    "print('Episode  Score')\n",
    "\n",
    "scores = []\n",
    "completionCount = 0 \n",
    "\n",
    "for episode in range(EPISODES):\n",
    "\n",
    "    current_state = tuple(env.reset()) \n",
    "    \n",
    "    # Updates learning rate and epsilon \n",
    "    learning_rate = get_learning_rate(episode)\n",
    "    epsilon = get_epsilon(episode)\n",
    "    \n",
    "    # Runs through an episode \n",
    "    done = False\n",
    "    while not done:\n",
    "        \n",
    "        action = choose_action(current_state)              # Chooses action\n",
    "        obs, reward, done, _ = env.step(action)            # Performs action \n",
    "        new_state = tuple(obs)                             # Discretizes new state\n",
    "        update_q(current_state, action, reward, new_state) # Updates the Q-Table\n",
    "        current_state = new_state                          # Updates the current state\n",
    "        \n",
    "        if reward == 10.0: completionCount += 1 \n",
    "     \n",
    "    # Prints some statistics  \n",
    "    if (episode + 1) % SHOW_STATS == 0: \n",
    "        score = round((completionCount / SHOW_STATS) * 100, 2)\n",
    "        completionCount = 0\n",
    "        print(f'{episode + 1}\\t {score}%') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0]\n",
      "[2, 0]\n",
      "[3, 0]\n",
      "[4, 0]\n",
      "[5, 0]\n",
      "[6, 0]\n",
      "[6, 1]\n",
      "[6, 2]\n",
      "[6, 3]\n",
      "[6, 4]\n",
      "[6, 5]\n",
      "[6, 6]\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.0 \n",
    "current_state = tuple(env.reset()) \n",
    "\n",
    "done = False \n",
    "while not done:\n",
    "        \n",
    "        # Chooses and performs action\n",
    "        action = choose_action(current_state)   \n",
    "        obs, reward, done, _ = env.step(action) \n",
    "        \n",
    "        # Sets new state\n",
    "        new_state = tuple(obs)\n",
    "        current_state = new_state    \n",
    "        \n",
    "        # Renders the frame \n",
    "        env.render(q_table)\n",
    "        print(obs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
